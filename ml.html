<!doctype html>
<html lang="en">

<head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-105607174-2"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-105607174-2');
    </script>
    <title>Caleb Kruse - AI/ML</title>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport"
        content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <!-- Bootstrap -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-GLhlTQ8iRABdZLl6O3oVMWSktQOp6b7In1Zl3/Jr59b6EGGoI1aFkw7cmDA6j6gD" crossorigin="anonymous">
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <!-- css. Last sheet takes priority -->
    <link rel="stylesheet" href="./css/style-v2.css">
    <link rel="stylesheet" href="./css/header-backgrounds.css">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-w76AqPfDkMBDXo30jS1Sgez6pr3x5MlQ1ZAGC+nuZB+EYdgRZgiwxhTBTkF7CXvN"
        crossorigin="anonymous"></script>
</head>

<body>

    <header class="ml-header">
        <div class="container-fluid menu">
            <div class="d-flex justify-content-end">
                <a href="/">
                    <button class="button-black">home</button>
                </a>
            </div>
        </div>
    </header>



    <div class="container">
        <div class="row justify-content-center">
            <div class="col-10 text-center">
                <h5 class="small-heading">projects with data</h5>
                <h3 class="page-heading">
                    caleb works on machine learning, ai, and data science
                </h3>
            </div>
        </div>
    </div>

    <div class="container-fluid card-container">
        <div class="row justify-content-center">
            <div class="col-lg-5 col-md-9 col-xs-12 content-card">
                <a href="./ml/embeddings/">
                    <img class="card-image" src="./ml/embeddings/preview.png">
                    <h3 class="card-title">what do embedding models see</h3>
                </a>
                <div class="card-text">
                    <p>Interactive experiments exploring how AI models classify and organize the world.</p>
                </div>
            </div>

            <div class="col-lg-5 col-md-9 col-xs-12 content-card">
                <a href="https://globalplasticwatch.org/">
                    <img class="card-image" src="./img/gpw-waste-site.jpg">
                    <h3 class="card-title">global plastic watch</h3>
                </a>
                <div class="card-text">
                    <p>
                        Plastic waste is a significant environmental pollutant that is difficult to monitor. We created
                        a system of neural networks to analyze spectral, spatial, and temporal components of Sentinel-2
                        satellite data to identify terrestrial aggregations of waste. The system works at wide
                        geographic scale, finding more than 4,000 waste sites in 112 countries across Southeast Asia.
                    </p>
                    <p>
                        The details of this work are published in the journal <a
                            href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0278997">PLOS
                            ONE.</a> Code and data is available on <a
                            href="https://github.com/earthrise-media/plastics">GitHub.</a>
                    </p>
                </div>
            </div>

            <div class="col-lg-5 col-md-9 col-xs-12 content-card">
                <a href="https://amazonminingwatch.org/">
                    <img class="card-image"
                        src="https://user-images.githubusercontent.com/13071901/146877405-3ec46c73-cc80-4b1a-8ad1-aeb189bb0b38.jpg">
                    <h3 class="card-title">amazon mining watch</h3>
                </a>
                <div class="card-text">
                    <p>
                        Amazon Mining Watch uses machine learning to map the scars of mining activities in the Amazonian
                        countries. By constantly analyzing high-resolution and historical satellite images, this tool
                        aims at identifying the fast-paced growth of open-pit mining in the largest rainforest in the
                        world. This database is here to help journalists, activists, and researchers better understand
                        the causes and impacts of the mining industry.
                    </p>
                    <p>
                        Code and methodology is available <a
                            href="https://github.com/earthrise-media/mining-detector">on GitHub.</a>
                    </p>
                </div>
            </div>

            <div class="col-lg-5 col-md-9 col-xs-12 content-card">
                <a href="./wikiclip/">
                    <img class="card-image"
                        src="./10-projects/wikiclip/img/bkg.jpg">
                    <h3 class="card-title">wikiclip</h3>
                </a>
                <div class="card-text">
                    <p>
                        A tool to find Wikipedia articles based on an image. I embedded 104,686 articles from Wikipedia and
                        created an embeddings database. I then query the database by creating a CLIP embedding of the image
                        and return the nearest articles. It's quite fun to play with!
                    </p>
                </div>
            </div>

            <div class="col-lg-5 col-md-9 col-xs-12 content-card">
                <a href="./old-faithful">
                    <img class="card-image" src="./img/IMG_9860.jpg">
                    <h3 class="card-title">old faithful</h3>
                </a>
                <div class="card-text">
                    <p>
                        Old Faithful is a geyser in Yellowstone National Park. After visiting Yellowstone in 2018, I
                        wondered if a neural network might be able to tease out more subtle correlations in the eruption
                        patterns to be able to forecast more accurately. Can we predict its next eruption using a neural
                        network?
                    </p>
                </div>
            </div>

            <div class="col-lg-5 col-md-9 col-sm-12 content-card">
                <a href="./sketches2legos">
                    <video class="card-image" autoplay loop muted>
                        <source src="img/edges2legos_man.mp4" type="video/mp4" />
                    </video>
                    <h3 class="card-title">sketches2legos</h3>
                </a>
                <div class="card-text">
                    <p>
                        Training a pix2pix image translation model to generate images of Lego sets from a user's
                        sketches. Follow the link to try edges2legos in the browser. The sketching experience is best
                        using a tablet and pencil/stylus.
                    </p>
                    <p>
                        Learn more about the datasets, model, and process <a href="#">here</a>
                    </p>
                </div>
            </div>

            <div class="col-lg-5 col-md-9 col-xs-12 content-card">
                <a href="https://twitter.com/clkruse/status/1227743392806785027/">
                    <img class="card-image" src="./img/deoldify_sem.jpg">
                    <h3 class="card-title">deoldify electron microscope</h3>
                </a>
                <div class="card-text">
                    <p>
                        This was a simple project to use learned image colorization (DeOldify) images from an electron
                        microscope, which can only produce grayscale outputs. Though the algorithm was heavily biased
                        towards flesh tones for these out-of-domain samples, certain structires can have quite striking
                        results. In the future, I would like to refine the DeOldify model by including some manually
                        colorized SEM images to the training set, or refine the colorization network to incorporate
                        color hints given by the user.
                    </p>
                </div>
            </div>

            <div class="col-lg-5 col-md-9 col-sm-12 content-card">
                <a href="https://github.com/laurelkruse/center-of-mouse">
                    <img class="card-image" src="./img/center_of_mouse.jpg"
                        style="border: 1px solid rgba(128, 128, 128, 0.5)">
                    <h3 class="card-title">center of mouse</h3>
                </a>
                <div class="card-text">
                    <p>
                        I was working with a friend studying neuroscience. Their lab spent a great deal of time watching
                        videos of mice in moving through experimental environments and recording their positions. I
                        wrote a simple algorithm to track the position of the mouse throughout the test environment and
                        compute basic statistics
                    </p>
                </div>
            </div>

            <div class="col-lg-5 col-md-9 col-sm-12 content-card">
                <a href="./sketches2legos">
                    <video class="card-image" autoplay loop muted>
                        <source src="img/legoshortsmall.mp4" type="video/mp4" />
                    </video>
                    <h3 class="card-title">lego sorter</h3>
                </a>
                <div class="card-text">
                    <p>
                        There are more than 10,000 different types of Lego bricks. Using simulated data generation in
                        Unity, can we develop a Lego brick classifier?
                    </p>
                </div>
            </div>

            <!-- Empty section for spacing -->
            <div class="col-lg-5 col-md-9 col-sm-12 content-card">

            </div>
        </div>
    </div>

    <footer id="footer" class="footer">
        <div class="row align-items-center">
            <div class="col-xs-12 col-6">
                <div class="footer-socials text-left">
                    <a href="https://twitter.com/clkruse" target="_blank">
                        <i class="fa fa-twitter"></i>
                    </a>
                    <a href="https://www.instagram.com/clkruse" target="_blank">
                        <i class="fa fa-instagram"></i>
                    </a>
                </div>
            </div>
            <div class="col-xs-12 col-6">
                <p class="copyright">Â© Caleb Kruse</p>
            </div>
        </div>
    </footer>
</body>

</html>